{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizers** are techniques or algorithms used to decrease loss (an error) by tuning various parameters and weights, hence minimizing the loss function, providing better accuracy of model faster."
      ],
      "metadata": {
        "id": "IIsvBvguSBXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "BuKkKikrRyGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIGfsLC2Ruyz"
      },
      "outputs": [],
      "source": [
        "#tf.train.Optimizer - Tensorflow version 1.x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.compat.v1.train.Optimizer - Tensorflow version 2.x"
      ],
      "metadata": {
        "id": "L8SXZRbpRxRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how optimizers work: they iteratively adjust the parameters of a model (like\n",
        "ùë•\n",
        ") to reduce the error (loss function) by following the direction of the gradient."
      ],
      "metadata": {
        "id": "mTbjX34gS_p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var = tf.Variable(2.0)\n",
        "\n",
        "# Use the experimental SGD optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "4kxuBvCKSFy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD updates the model's parameters by moving them in the direction opposite to the gradient of the loss function. It uses a constant learning rate, which you set manually"
      ],
      "metadata": {
        "id": "nkcR0b8AdAij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "    loss = var**2  # simple loss function"
      ],
      "metadata": {
        "id": "gS_UnSr7TSEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradients = tape.gradient(loss, [var])"
      ],
      "metadata": {
        "id": "DhxwpLR4UCrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.apply_gradients(zip(gradients, [var]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7732JNVUGSz",
        "outputId": "7dcf74cc-8be1-472d-bd28-f2345d6c200b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasVariable shape=(), dtype=int64, path=SGD/iteration>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var.numpy()"
      ],
      "metadata": {
        "id": "rkxAnK1aUJD0",
        "outputId": "2ea2a619-82e6-4f1a-d431-0c56cfed4ec5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.96"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "jiLkVc_EUMQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adam (Adaptive Moment Estimation):**\n",
        "\n",
        " Adam computes individual adaptive learning rates for different parameters based on estimates of first and second moments of the gradients.\n",
        "\n",
        " Adam is often the default choice for many neural networks due to its adaptability and generally good performance across a variety of tasks."
      ],
      "metadata": {
        "id": "SBgSm0wndTJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.apply_gradients(zip(gradients, [var]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWcXC4pEdFFI",
        "outputId": "4e482b0b-0800-41ee-8494-23e960ced5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasVariable shape=(), dtype=int64, path=adam/iteration>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej-ECmxNdPXD",
        "outputId": "f18f4b66-174a-457c-c340-43c180954553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9500002"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "6sRucVUVei2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RMSprop (Root Mean Square Propagation):**\n",
        "\n",
        "RMSprop works by dividing the learning rate by an exponentially decaying average of squared gradients.\n",
        "\n",
        "\n",
        "RMSprop is especially useful for RNNs (Recurrent Neural Networks) and other deep networks where you might encounter vanishing or exploding gradients."
      ],
      "metadata": {
        "id": "Zw3IBSdUeR9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.apply_gradients(zip(gradients, [var]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCDHHRyzdRnX",
        "outputId": "d1eb1868-c593-401c-8a05-54b97f4ecd09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasVariable shape=(), dtype=int64, path=rmsprop/iteration>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2obyK4S1el0V",
        "outputId": "490b3050-46b3-4b2e-c583-c72fada8e66c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9183774"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "eo3tZOEyenFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adagrad (Adaptive Gradient Algorithm):**\n",
        "\n",
        "Adagrad adapts the learning rate for each parameter based on the sum of the squares of the past gradients.\n",
        "\n",
        "Adagrad is useful for sparse data and natural language processing tasks where certain features appear infrequently."
      ],
      "metadata": {
        "id": "LNeEUmRQeqxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.apply_gradients(zip(gradients, [var]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf4nkgBhe4M2",
        "outputId": "2040f422-4eec-4b95-e3d6-e9e053fad359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasVariable shape=(), dtype=int64, path=adagrad/iteration>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMBhKY3ke8eW",
        "outputId": "5d45f9a9-7c81-4ddc-ed17-9e600cbb040e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9084085"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adadelta(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "IJNQ2NwOfche"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adadelta:**\n",
        "\n",
        "Adadelta is an extension of Adagrad designed to reduce its aggressive, monotonically decreasing learning rate.\n",
        "\n",
        "Adadelta is effective when you need to avoid the diminishing learning rate problem that Adagrad faces.\n",
        "It‚Äôs particularly useful in deep learning where you might not want to set a specific learning rate."
      ],
      "metadata": {
        "id": "9qs6NVMMfDGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.apply_gradients(zip(gradients, [var]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e9TQS2ce9cI",
        "outputId": "1f176817-e12c-4cd4-a28a-1e7369ac2a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasVariable shape=(), dtype=int64, path=adadelta/iteration>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX3E1Dxmfg7g",
        "outputId": "d6e0c577-63d9-4297-fbf6-4d73af99f97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9083943"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adamax(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "f83poUFvfh_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adamax:**\n",
        "\n",
        "Adamax is a variant of Adam based on the infinity norm. It can be more stable than Adam in certain cases because it uses the maximum absolute value of the gradients to update the parameters.\n",
        "\n",
        "\n",
        "Adamax can be useful in situations where the standard Adam optimizer is not performing well, particularly when dealing with very large datasets or models."
      ],
      "metadata": {
        "id": "M8yaVCMxflfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.apply_gradients(zip(gradients, [var]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJEWzwrhfvfI",
        "outputId": "fd0a98aa-4fd1-4815-85e9-21f30a1974ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasVariable shape=(), dtype=int64, path=adamax/iteration>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2SFFRUNf5mc",
        "outputId": "187b925b-d214-4007-d287-c70e917de5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.8983943"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "BPvNRBwVf7Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nadam (Nesterov-accelerated Adaptive Moment Estimation):**\n",
        "\n",
        "Nadam combines Adam with Nesterov momentum, which looks ahead at the next anticipated position before making the parameter update. This can lead to more stable and faster convergence.\n",
        "\n",
        "\n",
        "\n",
        "Nadam is beneficial when you need the stability and performance of Adam with the added benefit of momentum, especially in cases where models are sensitive to parameter updates."
      ],
      "metadata": {
        "id": "IwvPTWvff_Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.apply_gradients(zip(gradients, [var]))"
      ],
      "metadata": {
        "id": "taPJ32AEgLse",
        "outputId": "52635ec9-118b-41d4-9a34-228a2540e7ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasVariable shape=(), dtype=int64, path=nadam/iteration>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var.numpy()"
      ],
      "metadata": {
        "id": "_o_eLDOcgNMO",
        "outputId": "6db8d50e-5c12-4a53-8d20-f79be34b184a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.8877665"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}