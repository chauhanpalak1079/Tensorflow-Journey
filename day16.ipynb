{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Activation Functions:** They help neural networks learn and make sense of complex data."
      ],
      "metadata": {
        "id": "xRHmMs70xJqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An activation function in a neural network decides whether a neuron should be activated or not.\n",
        "It introduces non-linearity into the model, allowing it to learn complex patterns and relationships in data.\n",
        "\n",
        "Without activation functions, a neural network would behave like a linear model, limiting its ability to capture intricate patterns.\n",
        "\n",
        "Common activation functions include ReLU, Leaky ReLU, Softmax, and Softplus, each serving a specific purpose in the network's learning process."
      ],
      "metadata": {
        "id": "Cz2z3ihOxhrY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YSbr25lw5S4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([-2.0,-1.0, 0.0, 1.0, 2.0])"
      ],
      "metadata": {
        "id": "lsUzdmGlyIAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ReLU activation\n",
        "relu_x = tf.nn.relu(x)"
      ],
      "metadata": {
        "id": "7T-MlfVkydVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(relu_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlc5lzC4yi9o",
        "outputId": "e33a35e2-9e7b-4404-8bf7-cd44884147e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0. 0. 1. 2.], shape=(5,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: **ReLU** is one of the most popular activation functions. It replaces negative values with zero and keeps positive values unchanged."
      ],
      "metadata": {
        "id": "tV82U2dky0Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([-1.0, 0.0, 1.0, 2.0])"
      ],
      "metadata": {
        "id": "ruNQuucLyl_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Leaky ReLU activation with a negative slope (alpha)\n",
        "leaky_relu_x = tf.nn.leaky_relu(x, alpha=0.2)"
      ],
      "metadata": {
        "id": "md-GVUvjy8Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(leaky_relu_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuCqTZ42zCd4",
        "outputId": "7a68bb0f-f1e9-4dc8-d030-b72f09cac300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([-0.2  0.   1.   2. ], shape=(4,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: **Leaky ReLU** is similar to ReLU, but it allows a small, non-zero gradient for negative values. This can help with training by avoiding dead neurons."
      ],
      "metadata": {
        "id": "dowQrfhPzSfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = tf.constant([2.0, 1.0, 0.1]) #tensor with logits"
      ],
      "metadata": {
        "id": "nVLORD-kzFcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_x = tf.keras.layers.Softmax()(logits)"
      ],
      "metadata": {
        "id": "vPh66rU4z3Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(softmax_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sv3VPBWz4_u",
        "outputId": "c3ad52f9-9275-4324-8a30-2b53c1e2fc0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0.6590012  0.24243298 0.09856589], shape=(3,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: **Softmax** converts raw scores (logits) into probabilities. It’s typically used in the output layer of a classification model to predict class probabilities."
      ],
      "metadata": {
        "id": "hSC-HFGD0R0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([-2.0,-1.0, 0.0, 1.0, 2.0])"
      ],
      "metadata": {
        "id": "HZmqdmOPz5Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softplus_x = tf.nn.softplus(x)"
      ],
      "metadata": {
        "id": "ffF1D4eI1Fwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(softmax_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khCJUB5P1L6U",
        "outputId": "8613d44d-0ef3-49a9-c6f4-61c76f060b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0.6590012  0.24243298 0.09856589], shape=(3,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: **Softplus** is a smooth approximation to the ReLU function. It’s useful for creating a differentiable alternative to ReLU."
      ],
      "metadata": {
        "id": "-V2eZ_tZ1VhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary:**\n",
        "\n",
        "tf.nn.relu(): Simple and widely used; turns negative values to 0.\n",
        "\n",
        "tf.nn.leaky_relu(): Similar to ReLU, but allows a small gradient for negative values.\n",
        "\n",
        "tf.keras.layers.Softmax: Converts logits to probabilities for classification tasks.\n",
        "\n",
        "tf.nn.softplus(): A smooth version of ReLU, useful for continuous transitions."
      ],
      "metadata": {
        "id": "9d2Rt-vl1uyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#another example\n",
        "t1 = tf.constant([-3.5, -2.0, 0.5, 1.5, 3.0])\n",
        "\n",
        "# Apply ReLU activation\n",
        "relu_x = tf.nn.relu(t1)"
      ],
      "metadata": {
        "id": "XkU7M6AZ1PBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu_x.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABA0jf_g2U8c",
        "outputId": "26b3c884-da74-496f-d2cc-be2f3fe48307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0. , 0. , 0.5, 1.5, 3. ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t2 = tf.constant([-4.5, -3.0, 0.6, 1.8, 3.5])\n",
        "\n",
        "# Apply Leaky ReLU activation with a small slope (alpha)\n",
        "leaky_relu_x = tf.nn.leaky_relu(t2, alpha=0.5)"
      ],
      "metadata": {
        "id": "91si3ZnI2Xmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leaky_relu_x.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmxmT_vs2m0h",
        "outputId": "a66ce3d4-e4c4-4fc0-cb2b-d58c45c3cf41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.25, -1.5 ,  0.6 ,  1.8 ,  3.5 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t3 = tf.constant([8.0, 1.0, 0.5])\n",
        "\n",
        "# Apply Softmax activation\n",
        "softmax_x = tf.nn.softmax(t3)"
      ],
      "metadata": {
        "id": "kzn4s-YT2opi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_x.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDPq1BTf26Cr",
        "outputId": "46b88285-5aab-4dc8-9863-63e218424131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.985372e-01, 9.105480e-04, 5.522753e-04], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t4 = tf.constant([-3.5, -2.0, 0.5, 1.5, 3.0])\n",
        "\n",
        "# Apply Softplus activation\n",
        "softplus_x = tf.nn.softplus(t4)"
      ],
      "metadata": {
        "id": "MvM12uOR27hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softplus_x.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXum3XSF3RI_",
        "outputId": "0f122822-1a3a-4842-f724-ca64d162b6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.02975042, 0.126928  , 0.974077  , 1.7014133 , 3.0485873 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcmSc4a13VgR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}